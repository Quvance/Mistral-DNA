{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Raphael Mourad\n",
    "### Associate Professor\n",
    "### University Paul Sabatier / INRAE MIAT Lab Toulouse\n",
    "### 20/12/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to fine tune GPT-NEO on vignettes to predict recommendation for surgery.\n",
    "# GPT-Neo 1.3B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 16:53:05.234563: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-11 16:53:05.543554: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-11 16:53:06.184183: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib/x86_64-linux-gnu\n",
      "2024-01-11 16:53:06.185214: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib/x86_64-linux-gnu\n",
      "2024-01-11 16:53:06.185221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPTNeoForSequenceClassification, GPT2ForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from peft import PeftModel, PeftConfig, LoftQConfig\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, AutoModelForPreTraining\n",
    "from transformers import AutoModelForMaskedLM, CharacterTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwitchTransformersForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): SwitchTransformersStack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersDenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersSparseMLP(\n",
       "              (router): SwitchTransformersTop1Router(\n",
       "                (classifier): Linear(in_features=768, out_features=8, bias=False)\n",
       "              )\n",
       "              (experts): ModuleDict(\n",
       "                (expert_0): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_1): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_2): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_3): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_4): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_5): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_6): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_7): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersDenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersSparseMLP(\n",
       "              (router): SwitchTransformersTop1Router(\n",
       "                (classifier): Linear(in_features=768, out_features=8, bias=False)\n",
       "              )\n",
       "              (experts): ModuleDict(\n",
       "                (expert_0): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_1): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_2): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_3): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_4): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_5): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_6): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_7): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersDenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersSparseMLP(\n",
       "              (router): SwitchTransformersTop1Router(\n",
       "                (classifier): Linear(in_features=768, out_features=8, bias=False)\n",
       "              )\n",
       "              (experts): ModuleDict(\n",
       "                (expert_0): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_1): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_2): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_3): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_4): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_5): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_6): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_7): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersDenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersSparseMLP(\n",
       "              (router): SwitchTransformersTop1Router(\n",
       "                (classifier): Linear(in_features=768, out_features=8, bias=False)\n",
       "              )\n",
       "              (experts): ModuleDict(\n",
       "                (expert_0): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_1): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_2): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_3): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_4): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_5): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_6): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_7): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersDenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersSparseMLP(\n",
       "              (router): SwitchTransformersTop1Router(\n",
       "                (classifier): Linear(in_features=768, out_features=8, bias=False)\n",
       "              )\n",
       "              (experts): ModuleDict(\n",
       "                (expert_0): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_1): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_2): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_3): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_4): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_5): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_6): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_7): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersDenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersSparseMLP(\n",
       "              (router): SwitchTransformersTop1Router(\n",
       "                (classifier): Linear(in_features=768, out_features=8, bias=False)\n",
       "              )\n",
       "              (experts): ModuleDict(\n",
       "                (expert_0): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_1): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_2): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_3): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_4): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_5): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_6): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_7): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): SwitchTransformersLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): SwitchTransformersStack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerCrossAttention(\n",
       "            (EncDecAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersDenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerCrossAttention(\n",
       "            (EncDecAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersSparseMLP(\n",
       "              (router): SwitchTransformersTop1Router(\n",
       "                (classifier): Linear(in_features=768, out_features=8, bias=False)\n",
       "              )\n",
       "              (experts): ModuleDict(\n",
       "                (expert_0): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_1): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_2): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_3): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_4): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_5): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_6): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_7): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerCrossAttention(\n",
       "            (EncDecAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersDenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerCrossAttention(\n",
       "            (EncDecAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersSparseMLP(\n",
       "              (router): SwitchTransformersTop1Router(\n",
       "                (classifier): Linear(in_features=768, out_features=8, bias=False)\n",
       "              )\n",
       "              (experts): ModuleDict(\n",
       "                (expert_0): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_1): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_2): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_3): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_4): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_5): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_6): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_7): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerCrossAttention(\n",
       "            (EncDecAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersDenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerCrossAttention(\n",
       "            (EncDecAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersSparseMLP(\n",
       "              (router): SwitchTransformersTop1Router(\n",
       "                (classifier): Linear(in_features=768, out_features=8, bias=False)\n",
       "              )\n",
       "              (experts): ModuleDict(\n",
       "                (expert_0): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_1): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_2): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_3): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_4): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_5): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_6): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_7): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerCrossAttention(\n",
       "            (EncDecAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersDenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerCrossAttention(\n",
       "            (EncDecAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersSparseMLP(\n",
       "              (router): SwitchTransformersTop1Router(\n",
       "                (classifier): Linear(in_features=768, out_features=8, bias=False)\n",
       "              )\n",
       "              (experts): ModuleDict(\n",
       "                (expert_0): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_1): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_2): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_3): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_4): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_5): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_6): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_7): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerCrossAttention(\n",
       "            (EncDecAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersDenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerCrossAttention(\n",
       "            (EncDecAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersSparseMLP(\n",
       "              (router): SwitchTransformersTop1Router(\n",
       "                (classifier): Linear(in_features=768, out_features=8, bias=False)\n",
       "              )\n",
       "              (experts): ModuleDict(\n",
       "                (expert_0): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_1): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_2): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_3): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_4): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_5): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_6): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_7): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerCrossAttention(\n",
       "            (EncDecAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersDenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): SwitchTransformersBlock(\n",
       "        (layer): ModuleList(\n",
       "          (0): SwitchTransformersLayerSelfAttention(\n",
       "            (SelfAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): SwitchTransformersLayerCrossAttention(\n",
       "            (EncDecAttention): SwitchTransformersAttention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): SwitchTransformersLayerFF(\n",
       "            (mlp): SwitchTransformersSparseMLP(\n",
       "              (router): SwitchTransformersTop1Router(\n",
       "                (classifier): Linear(in_features=768, out_features=8, bias=False)\n",
       "              )\n",
       "              (experts): ModuleDict(\n",
       "                (expert_0): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_1): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_2): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_3): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_4): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_5): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_6): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (expert_7): SwitchTransformersDenseActDense(\n",
       "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (layer_norm): SwitchTransformersLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): SwitchTransformersLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "config = AutoConfig.from_pretrained(\"google/switch-base-8\")\n",
    "model = AutoModelForPreTraining.from_config(config)\n",
    "#model = AutoModelForCausalLM.from_config(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DNABERT2\n",
    "config = AutoConfig.from_pretrained(\"/media/mourad/SSD2/MistralDNA/data/DNABERT-2-117M/\")\n",
    "#model = AutoModelForPreTraining.from_config(config)\n",
    "model = AutoModelForMaskedLM.from_config(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32489728"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Tokenizer\n",
    "\n",
    "\"\"\"\n",
    "Just a simple character level tokenizer.\n",
    "\n",
    "From: https://github.com/dariush-bahrami/character-tokenizer/blob/master/charactertokenizer/core.py\n",
    "\n",
    "CharacterTokenzier for Hugging Face Transformers.\n",
    "This is heavily inspired from CanineTokenizer in transformers package.\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=sep_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_side=padding_side,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [42], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m chars \u001b[38;5;241m=\u001b[39m string\u001b[38;5;241m.\u001b[39mascii_letters \u001b[38;5;66;03m# This character vocab!\u001b[39;00m\n\u001b[1;32m      9\u001b[0m model_max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m\n\u001b[0;32m---> 10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mCharacterTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [36], line 48\u001b[0m, in \u001b[0;36mCharacterTokenizer.__init__\u001b[0;34m(self, characters, model_max_length, padding_side, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m unk_token \u001b[38;5;241m=\u001b[39m AddedToken(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m mask_token \u001b[38;5;241m=\u001b[39m AddedToken(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_max_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vocab_str_to_int \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{ch: i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(characters)},\n\u001b[1;32m     71\u001b[0m }\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vocab_int_to_str \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vocab_str_to_int\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py:367\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# 4. If some of the special tokens are not part of the vocab, we add them, at the end.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens_extended\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_added_tokens_encoder\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspecial_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils.py:467\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._add_tokens\u001b[0;34m(self, new_tokens, special_tokens)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m added_tokens\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# TODO this is fairly slow to improve!\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m current_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    468\u001b[0m new_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(current_vocab)  \u001b[38;5;66;03m# only call this once, len gives the last index + 1\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m new_tokens:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1675\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vocab\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   1666\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1667\u001b[0m \u001b[38;5;124;03m    Returns the vocabulary as a dictionary of token to index.\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;124;03m        `Dict[str, int]`: The vocabulary.\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1675\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import string\n",
    "#from charactertokenizer import CharacterTokenizer\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/media/mourad/SSD2/MistralDNA/data/character-tokenizer/\")\n",
    "import charactertokenizer\n",
    "\n",
    "chars = string.ascii_letters # This character vocab!\n",
    "model_max_length = 2048\n",
    "tokenizer = CharacterTokenizer(chars, model_max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CharacterTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "#prompt = \"My favourite condiment is\"\n",
    "\n",
    "#model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "#model.to(device)\n",
    "\n",
    "#generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "\n",
    "#tokenizer.batch_decode(generated_ids)[0]\n",
    "#\"The expected output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IMPORT AND BUILD GPT-NEO\n",
    "# Define the task\n",
    "task_name = \"news_classification\"\n",
    "num_labels = 2\n",
    "model_name=\"GPTNEO2.7B\" # \"GPTNEO125m\" \"GPTNEO2.7B\" \"BioMedLM\"\n",
    "\n",
    "# Load the tokenizer\n",
    "if model_name==\"GPTNEO125m\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\") \n",
    "    model = GPTNeoForSequenceClassification.from_pretrained('EleutherAI/gpt-neo-125m')\n",
    "elif model_name==\"GPTNEO2.7B\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\") \n",
    "    model = GPTNeoForSequenceClassification.from_pretrained('EleutherAI/gpt-neo-2.7B')\n",
    "elif model_name==\"BioMedLM\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"stanford-crfm/BioMedLM\") \n",
    "    model = GPT2ForSequenceClassification.from_pretrained('stanford-crfm/BioMedLM')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the pre-trained GPT-3 model\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# Add a classification head on top of the model\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, num_labels)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PARAMETERS FOR FINE-TUNING\n",
    "if model_name!=\"GPTNEO125m\":\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results/'+model_name,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        learning_rate=1e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=True,\n",
    "        gradient_accumulation_steps=10,\n",
    "    )\n",
    "elif model_name==\"GPTNEO125m\": # For small models\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results/'+model_name,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        num_train_epochs=50,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=1e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=True,\n",
    "        fp16=True,\n",
    "    )\n",
    "\n",
    "print(training_args)\n",
    "\n",
    "#loftq_config = LoftQConfig(loftq_bits=4)\n",
    "lora_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False,\n",
    "    #init_lora_weights=\"loftq\", \n",
    "    #loftq_config=loftq_config,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FINE-TUNE MODEL ON VIGNETTES\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Deploy the model\n",
    "model.save_pretrained(model_name+'/vignette_classification_model')\n",
    "tokenizer.save_pretrained(model_name+'/vignette_classification_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
